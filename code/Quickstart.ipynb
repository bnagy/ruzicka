{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ružička: Authorship Verification in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we offer a quick tutorial as to how you could use the code in this repository. While the package is very much geared towards our own work in authorship verification, you might some of the more general functions useful. All feedback and comments are welcome. This code assumes Python 2.7+ (Python 3 has not been tested). You do not need to install the library to run the code below, but please note that there are a number of well-known third-party Python libraries, including:\n",
    "+ numpy\n",
    "+ scipy\n",
    "+ scikit-learn\n",
    "+ matplotlib\n",
    "+ seaborn\n",
    "+ numba\n",
    "\n",
    "and preferably (for GPU acceleration and/or JIT-compilation):\n",
    "+ theano\n",
    "+ numbapro\n",
    "\n",
    "We recommend installing Continuum's excellent [Anaconda Python framework](https://www.continuum.io/downloads), which comes bundled with most of these dependencies.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk through"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we assume that your data sets are stored in a directory the format on the PAN 2014 track on authorship attribution: a directory should minimally include one folder per verification problem (an `unknown.txt` and at least one `known01.txt`) and a `truth.txt`. E.g. for the corpus of Dutch essays (`../data/2014/du_essays/train`), `truth.txt` contains has a tab-separated line with the ground truth for each problem:\n",
    "\n",
    "```\n",
    "DE001 Y\n",
    "DE002 Y\n",
    "DE003 N\n",
    "DE004 N\n",
    "DE005 N\n",
    "DE006 N\n",
    "DE007 N\n",
    "DE008 Y\n",
    "...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDE001\u001b[m\u001b[m         \u001b[34mDE021\u001b[m\u001b[m         \u001b[34mDE041\u001b[m\u001b[m         \u001b[34mDE061\u001b[m\u001b[m         \u001b[34mDE081\u001b[m\u001b[m\n",
      "\u001b[34mDE002\u001b[m\u001b[m         \u001b[34mDE022\u001b[m\u001b[m         \u001b[34mDE042\u001b[m\u001b[m         \u001b[34mDE062\u001b[m\u001b[m         \u001b[34mDE082\u001b[m\u001b[m\n",
      "\u001b[34mDE003\u001b[m\u001b[m         \u001b[34mDE023\u001b[m\u001b[m         \u001b[34mDE043\u001b[m\u001b[m         \u001b[34mDE063\u001b[m\u001b[m         \u001b[34mDE083\u001b[m\u001b[m\n",
      "\u001b[34mDE004\u001b[m\u001b[m         \u001b[34mDE024\u001b[m\u001b[m         \u001b[34mDE044\u001b[m\u001b[m         \u001b[34mDE064\u001b[m\u001b[m         \u001b[34mDE084\u001b[m\u001b[m\n",
      "\u001b[34mDE005\u001b[m\u001b[m         \u001b[34mDE025\u001b[m\u001b[m         \u001b[34mDE045\u001b[m\u001b[m         \u001b[34mDE065\u001b[m\u001b[m         \u001b[34mDE085\u001b[m\u001b[m\n",
      "\u001b[34mDE006\u001b[m\u001b[m         \u001b[34mDE026\u001b[m\u001b[m         \u001b[34mDE046\u001b[m\u001b[m         \u001b[34mDE066\u001b[m\u001b[m         \u001b[34mDE086\u001b[m\u001b[m\n",
      "\u001b[34mDE007\u001b[m\u001b[m         \u001b[34mDE027\u001b[m\u001b[m         \u001b[34mDE047\u001b[m\u001b[m         \u001b[34mDE067\u001b[m\u001b[m         \u001b[34mDE087\u001b[m\u001b[m\n",
      "\u001b[34mDE008\u001b[m\u001b[m         \u001b[34mDE028\u001b[m\u001b[m         \u001b[34mDE048\u001b[m\u001b[m         \u001b[34mDE068\u001b[m\u001b[m         \u001b[34mDE088\u001b[m\u001b[m\n",
      "\u001b[34mDE009\u001b[m\u001b[m         \u001b[34mDE029\u001b[m\u001b[m         \u001b[34mDE049\u001b[m\u001b[m         \u001b[34mDE069\u001b[m\u001b[m         \u001b[34mDE089\u001b[m\u001b[m\n",
      "\u001b[34mDE010\u001b[m\u001b[m         \u001b[34mDE030\u001b[m\u001b[m         \u001b[34mDE050\u001b[m\u001b[m         \u001b[34mDE070\u001b[m\u001b[m         \u001b[34mDE090\u001b[m\u001b[m\n",
      "\u001b[34mDE011\u001b[m\u001b[m         \u001b[34mDE031\u001b[m\u001b[m         \u001b[34mDE051\u001b[m\u001b[m         \u001b[34mDE071\u001b[m\u001b[m         \u001b[34mDE091\u001b[m\u001b[m\n",
      "\u001b[34mDE012\u001b[m\u001b[m         \u001b[34mDE032\u001b[m\u001b[m         \u001b[34mDE052\u001b[m\u001b[m         \u001b[34mDE072\u001b[m\u001b[m         \u001b[34mDE092\u001b[m\u001b[m\n",
      "\u001b[34mDE013\u001b[m\u001b[m         \u001b[34mDE033\u001b[m\u001b[m         \u001b[34mDE053\u001b[m\u001b[m         \u001b[34mDE073\u001b[m\u001b[m         \u001b[34mDE093\u001b[m\u001b[m\n",
      "\u001b[34mDE014\u001b[m\u001b[m         \u001b[34mDE034\u001b[m\u001b[m         \u001b[34mDE054\u001b[m\u001b[m         \u001b[34mDE074\u001b[m\u001b[m         \u001b[34mDE094\u001b[m\u001b[m\n",
      "\u001b[34mDE015\u001b[m\u001b[m         \u001b[34mDE035\u001b[m\u001b[m         \u001b[34mDE055\u001b[m\u001b[m         \u001b[34mDE075\u001b[m\u001b[m         \u001b[34mDE095\u001b[m\u001b[m\n",
      "\u001b[34mDE016\u001b[m\u001b[m         \u001b[34mDE036\u001b[m\u001b[m         \u001b[34mDE056\u001b[m\u001b[m         \u001b[34mDE076\u001b[m\u001b[m         \u001b[34mDE096\u001b[m\u001b[m\n",
      "\u001b[34mDE017\u001b[m\u001b[m         \u001b[34mDE037\u001b[m\u001b[m         \u001b[34mDE057\u001b[m\u001b[m         \u001b[34mDE077\u001b[m\u001b[m         \u001b[31mcontents.json\u001b[m\u001b[m\n",
      "\u001b[34mDE018\u001b[m\u001b[m         \u001b[34mDE038\u001b[m\u001b[m         \u001b[34mDE058\u001b[m\u001b[m         \u001b[34mDE078\u001b[m\u001b[m         \u001b[31mtruth.json\u001b[m\u001b[m\n",
      "\u001b[34mDE019\u001b[m\u001b[m         \u001b[34mDE039\u001b[m\u001b[m         \u001b[34mDE059\u001b[m\u001b[m         \u001b[34mDE079\u001b[m\u001b[m         \u001b[31mtruth.txt\u001b[m\u001b[m\n",
      "\u001b[34mDE020\u001b[m\u001b[m         \u001b[34mDE040\u001b[m\u001b[m         \u001b[34mDE060\u001b[m\u001b[m         \u001b[34mDE080\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/2014/du_essays/train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load the set of development problems for the Dutch essays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ruzicka.utilities import *\n",
    "\n",
    "D = \"../data/2014/du_essays/\"\n",
    "dev_train_data, dev_test_data = load_pan_dataset(D + \"train\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions loads all documents and splits the development data into a development part (the known documents) and a testing part (the unknown documents). We can unpack these as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_train_labels, dev_train_documents = zip(*dev_train_data)\n",
    "dev_test_labels, dev_test_documents = zip(*dev_test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the actual test texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+  ﻿Dankzij het internet zijn we een grote bron aan informatie rijker . A\n",
      "+  ﻿Het is dus begrijpelijk dat de commerciële zenders meer reclame moete\n",
      "+  ﻿\" Hey , vuile nicht ! Hangt er nog stront aan je lul ? \" . Dergelijke\n",
      "+  ﻿Gelijkheid tussen man en vrouw is iets dat ons al eeuwen in de ban ho\n",
      "+  ﻿Gisteren was er opnieuw een protest tegen homofilie in de grootstad P\n",
      "+  ﻿Voetbal is vandaag de dag zonder twijfel de populairste sport in Belg\n",
      "+  ﻿Door de ongekende groei van nieuwsbronnen en de opkomst van het inter\n",
      "+  ﻿Woordenboekgebruik uit interesse De categorie woordenboekgebruikers d\n",
      "+  ﻿Ze bouwden een tegencultuur op die alles verwierp waar hun ouders alt\n",
      "+  ﻿Als we hier in België op straat rondlopen , merken we dat er zeer vee\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "for doc in dev_test_documents[:10]:\n",
    "    print(\"+ \", doc[:70])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of these documents we need to decide whether or not they were in fact written by the target authors proposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+  DE001\n",
      "+  DE002\n",
      "+  DE003\n",
      "+  DE004\n",
      "+  DE005\n",
      "+  DE006\n",
      "+  DE007\n",
      "+  DE008\n",
      "+  DE009\n",
      "+  DE010\n"
     ]
    }
   ],
   "source": [
    "for doc in dev_test_labels[:10]:\n",
    "    print(\"+ \", doc[:70])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and crucial step is to vectorize the documents using a vector space model. Below, we use generic example, using the 10,000 most common word unigrams and a plain *tf* model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/venv/py310/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from ruzicka.vectorization import Vectorizer\n",
    "\n",
    "vectorizer = Vectorizer(mfi=10000, vector_space=\"tf\", ngram_type=\"word\", ngram_size=1)\n",
    "\n",
    "dev_train_X = vectorizer.fit_transform(dev_train_documents)\n",
    "dev_test_X = vectorizer.transform(dev_test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_test_X.__class__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use `sklearn` conventions here: we fit the vectorizer only on the vocabulary of the known documents and apply it it later to the unknown documents (since in real life too, we will not necessarily know the known documents in advance). This gives us two compatible corpus matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 9347)\n",
      "(96, 9347)\n"
     ]
    }
   ],
   "source": [
    "print(dev_train_X.shape)\n",
    "print(dev_test_X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now encode the author labels in the development problem sets as integers, using sklearn's convenient `LabelEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(dev_train_labels + dev_test_labels)\n",
    "dev_train_y = np.array(label_encoder.transform(dev_train_labels))\n",
    "dev_test_y = np.array(label_encoder.transform(dev_test_labels))\n",
    "print(dev_test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now construct and fit an 'O2' verifier: this extrinsic verification technique is based on the General Imposters framework. We apply it with the minmax metric and a profile base, meaning that the known documents for each author will be represented as a mean centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ruzicka.Order2Verifier import Order2Verifier\n",
    "\n",
    "dev_verifier = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"profile\", nb_bootstrap_iter=100, rnd_prop=0.5\n",
    ")\n",
    "dev_verifier.fit(dev_train_X, dev_train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain the probability which this O1 verifier would assign to each combination of an unknown document and the target author suggested in the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - # test documents processed: 10 out of 96\n",
      "\t - # test documents processed: 20 out of 96\n",
      "\t - # test documents processed: 30 out of 96\n",
      "\t - # test documents processed: 40 out of 96\n",
      "\t - # test documents processed: 50 out of 96\n",
      "\t - # test documents processed: 60 out of 96\n",
      "\t - # test documents processed: 70 out of 96\n",
      "\t - # test documents processed: 80 out of 96\n",
      "\t - # test documents processed: 90 out of 96\n"
     ]
    }
   ],
   "source": [
    "dev_test_scores = dev_verifier.predict_proba(\n",
    "    test_X=dev_test_X, test_y=dev_test_y, nb_imposters=30\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us as an array of probability scores for each problem, corresponding to the number of iterations in which the target's author's profile was closer to the anonymous document than to one of the imposters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79 0.69 0.01 0.   0.08 0.04 0.02 1.   0.99 0.69 0.51 0.28 0.88 0.97\n",
      " 0.01 0.58 0.62 0.   0.01 0.34 0.   0.   0.59 0.43 0.   0.76 0.03 0.53\n",
      " 0.03 0.01 0.01 0.   0.28 0.   0.   0.26 0.   0.04 0.04 0.   0.12 0.81\n",
      " 0.   0.74 0.99 0.   0.34 0.03 0.   0.2  0.02 0.45 0.07 0.47 0.65 0.\n",
      " 0.01 0.71 0.02 0.69 0.98 0.65 0.92 0.46 0.54 0.98 0.81 0.12 0.5  0.91\n",
      " 0.28 0.01 0.01 0.4  0.07 0.15 0.24 0.   0.97 0.   0.89 0.26 0.   0.\n",
      " 0.99 0.   1.   0.62 0.   0.   0.   0.6  0.   0.   0.45 0.71]\n"
     ]
    }
   ],
   "source": [
    "print(dev_test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load the ground truth to check how well we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "dev_gt_scores = load_ground_truth(\n",
    "    filepath=os.sep.join((D, \"train\", \"truth.txt\")), labels=dev_test_labels\n",
    ")\n",
    "print(dev_gt_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one final step needed: the PAN evaluation measures allow systems to leave a number of difficult problems unanswered, by setting the probability exactly at 0.5. To account for this strict threshold, we fit a score shifter, which will attempt to rectify mid-range score to 0.5. We can tune these parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 for optimal combo: 0.08000000000000002\n",
      "p2 for optimal combo: 0.29000000000000004\n",
      "AUC for optimal combo: 0.9461805555555556\n",
      "c@1 for optimal combo: 0.931640625\n"
     ]
    }
   ],
   "source": [
    "from ruzicka.score_shifting import ScoreShifter\n",
    "\n",
    "shifter = ScoreShifter()\n",
    "shifter.fit(predicted_scores=dev_test_scores, ground_truth_scores=dev_gt_scores)\n",
    "dev_test_scores = shifter.transform(dev_test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this shifter optimizes 2 parameters using a grid search: all values in between *p1* and *p2* will be rectified to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8509, 0.7799, 0.0007999999999999996, 0.0, 0.006399999999999997, 0.0031999999999999984, 0.0015999999999999992, 1.0, 0.9929, 0.7799, 0.6520999999999999, 0.5, 0.9148000000000001, 0.9787, 0.0007999999999999996, 0.7018, 0.7302, 0.0, 0.0007999999999999996, 0.5314, 0.0, 0.0, 0.7088999999999999, 0.5952999999999999, 0.0, 0.8295999999999999, 0.002399999999999999, 0.6662999999999999, 0.002399999999999999, 0.0007999999999999996, 0.0007999999999999996, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0031999999999999984, 0.0031999999999999984, 0.0, 0.5, 0.8651, 0.0, 0.8154, 0.9929, 0.0, 0.5314, 0.002399999999999999, 0.0, 0.5, 0.0015999999999999992, 0.6094999999999999, 0.005599999999999998, 0.6236999999999999, 0.7515, 0.0, 0.0007999999999999996, 0.7941, 0.0015999999999999992, 0.7799, 0.9858, 0.7515, 0.9431999999999999, 0.6166, 0.6734, 0.9858, 0.8651, 0.5, 0.645, 0.9361, 0.5, 0.0007999999999999996, 0.0007999999999999996, 0.574, 0.005599999999999998, 0.5, 0.5, 0.0, 0.9787, 0.0, 0.9219, 0.5, 0.0, 0.0, 0.9929, 0.0, 1.0, 0.7302, 0.0, 0.0, 0.0, 0.716, 0.0, 0.0, 0.6094999999999999, 0.7941]\n"
     ]
    }
   ],
   "source": [
    "print(dev_test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can later apply this optimized score shifter to the test problems. Now the main question: how well would our O2 verifier perform on the development problems, given the optimal *p1* and *p2* found? We answer this question using the three evaluation measures used in the PAN competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.875\n",
      "AUC:  0.9461805555555556\n",
      "c@1:  0.931640625\n",
      "AUC x c@1:  0.881500244140625\n"
     ]
    }
   ],
   "source": [
    "from ruzicka.evaluation import pan_metrics\n",
    "\n",
    "dev_acc_score, dev_auc_score, dev_c_at_1_score = pan_metrics(\n",
    "    prediction_scores=dev_test_scores, ground_truth_scores=dev_gt_scores\n",
    ")\n",
    "print(\"Accuracy: \", dev_acc_score)\n",
    "print(\"AUC: \", dev_auc_score)\n",
    "print(\"c@1: \", dev_c_at_1_score)\n",
    "print(\"AUC x c@1: \", dev_auc_score * dev_c_at_1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our score shifting approach clearly pays off, since we are able to leave difficult problems unswered, yielding to a higher c@1 than pure accuracy. We can now proceed to the test problems. The following code block runs entire parallel to the approach above: only the score shifter isn't retrained again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/venv/py310/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t - # test documents processed: 10 out of 96\n",
      "\t - # test documents processed: 20 out of 96\n",
      "\t - # test documents processed: 30 out of 96\n",
      "\t - # test documents processed: 40 out of 96\n",
      "\t - # test documents processed: 50 out of 96\n",
      "\t - # test documents processed: 60 out of 96\n",
      "\t - # test documents processed: 70 out of 96\n",
      "\t - # test documents processed: 80 out of 96\n",
      "\t - # test documents processed: 90 out of 96\n",
      "Accuracy:  0.9166666666666666\n",
      "AUC:  0.9696180555555556\n",
      "c@1:  0.931640625\n",
      "AUC x c@1:  0.9033355712890625\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_pan_dataset(D + \"test\")\n",
    "train_labels, train_documents = zip(*train_data)\n",
    "test_labels, test_documents = zip(*test_data)\n",
    "\n",
    "# vectorize:\n",
    "vectorizer = Vectorizer(mfi=10000, vector_space=\"tf\", ngram_type=\"word\", ngram_size=1)\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)\n",
    "\n",
    "# encode author labels:\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels + test_labels)\n",
    "train_y = np.array(label_encoder.transform(train_labels), dtype=\"int\")\n",
    "test_y = np.array(label_encoder.transform(test_labels), dtype=\"int\")\n",
    "\n",
    "# fit and predict a verifier on the test data:\n",
    "test_verifier = Order2Verifier(\n",
    "    metric=\"minmax\", base=\"profile\", nb_bootstrap_iter=100, rnd_prop=0.5\n",
    ")\n",
    "test_verifier.fit(train_X, train_y)\n",
    "test_scores = test_verifier.predict_proba(\n",
    "    test_X=test_X, test_y=np.array(test_y), nb_imposters=30\n",
    ")\n",
    "\n",
    "# load the ground truth:\n",
    "test_gt_scores = load_ground_truth(\n",
    "    filepath=os.sep.join((D, \"test\", \"truth.txt\")), labels=test_labels\n",
    ")\n",
    "\n",
    "# apply the optimzed score shifter:\n",
    "test_scores = shifter.transform(test_scores)\n",
    "\n",
    "test_acc_score, test_auc_score, test_c_at_1_score = pan_metrics(\n",
    "    prediction_scores=test_scores, ground_truth_scores=test_gt_scores\n",
    ")\n",
    "\n",
    "print(\"Accuracy: \", test_acc_score)\n",
    "print(\"AUC: \", test_auc_score)\n",
    "print(\"c@1: \", test_c_at_1_score)\n",
    "print(\"AUC x c@1: \", test_auc_score * test_c_at_1_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our final test results are a bit lower, the verifier seems to scale reasonably well to the unseen verification problems in the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order Verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting now to compare the GI approach to a first-order verification system, which often yields very competitive results too. Our implementation closely resembles the system proposed by Potha and Stamatatos in 2014 (A Profile-based Method for Authorship Verification). We import and fit this O1 verifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70170134 0.7270711  0.37633175 0.28879446 0.49040943 0.42713267\n",
      " 0.38902158 0.8981929  0.8610844  0.634507   0.5582054  0.5116984\n",
      " 0.78004104 0.7290129  0.23379725 0.6144281  0.30090123 0.15461487\n",
      " 0.37763566 0.50886804 0.3619166  0.20511049 0.56136984 0.43511206\n",
      " 0.3470927  0.47622222 0.2909667  0.42521244 0.33016068 0.28262693\n",
      " 0.29360873 0.2508412  0.40444332 0.22095674 0.27538615 0.49316198\n",
      " 0.1846866  0.4193539  0.1801241  0.3017376  0.4689892  0.6709575\n",
      " 0.34283203 0.6015002  0.8864545  0.10788292 0.4630553  0.39002222\n",
      " 0.2620769  0.2199009  0.3541656  0.3327778  0.38411123 0.23165005\n",
      " 0.6096738  0.17532307 0.44518214 0.6944714  0.37893575 0.62718385\n",
      " 0.5080187  0.6302462  0.8078237  0.581901   0.53879637 0.72632724\n",
      " 0.61635345 0.36859185 0.6126836  0.76257414 0.11920172 0.\n",
      " 0.01431888 0.4648263  0.20724541 0.05414206 0.32293087 0.27691644\n",
      " 0.78255326 0.253038   0.49909264 0.4017859  0.24249572 0.2609076\n",
      " 0.85132855 0.23575276 1.         0.62689704 0.22979039 0.10448259\n",
      " 0.2666089  0.48809344 0.31457692 0.15830523 0.528109   0.5394768 ]\n"
     ]
    }
   ],
   "source": [
    "from ruzicka.Order1Verifier import Order1Verifier\n",
    "\n",
    "dev_verifier = Order1Verifier(metric=\"minmax\", base=\"profile\")\n",
    "dev_verifier.fit(dev_train_X, dev_train_y)\n",
    "dev_test_scores = dev_verifier.predict_proba(test_X=dev_test_X, test_y=dev_test_y)\n",
    "print(dev_test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, the 'probabilities' returned are only distance-based pseudo-probabilities and don't lie in the range of 0-1. Applying the score shifter is therefore quintessential with O1, since it will scale the distances to a more useful range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1 for optimal combo: 0.4000000000000001\n",
      "p2 for optimal combo: 0.4700000000000001\n",
      "AUC for optimal combo: 0.904513888888889\n",
      "c@1 for optimal combo: 0.8741319444444444\n",
      "[0.8419017118215562, 0.8553476864099503, 0.15053269863128665, 0.11551778316497804, 0.7299169999361039, 0.5, 0.15560863018035892, 0.9460422277450562, 0.9263747328519821, 0.8062887102365495, 0.7658488756418229, 0.7412001651525497, 0.8834217506647111, 0.8563768404722214, 0.09351890087127687, 0.7956468945741654, 0.12036049365997317, 0.06184594631195069, 0.15105426311492923, 0.7397000604867936, 0.144766640663147, 0.08204419612884523, 0.7675260132551194, 0.5, 0.1388370752334595, 0.7223977750539781, 0.11638667583465578, 0.5, 0.13206427097320558, 0.11305077075958254, 0.11744349002838136, 0.10033648014068605, 0.5, 0.08838269710540773, 0.11015446186065676, 0.7313758474588394, 0.07387464046478273, 0.5, 0.0720496416091919, 0.12069504261016847, 0.5, 0.8256074780225754, 0.1371328115463257, 0.7887951129674912, 0.9398208969831467, 0.043153166770935066, 0.5, 0.1560088872909546, 0.10483076572418215, 0.08796036243438722, 0.1416662454605103, 0.13311111927032473, 0.1536444902420044, 0.0926600217819214, 0.7931271129846573, 0.07012922763824464, 0.5, 0.8380698519945144, 0.15157430171966554, 0.8024074429273605, 0.7392498964071275, 0.8040304976701736, 0.8981465703248979, 0.7784075373411179, 0.7555620735883712, 0.8549534374475479, 0.7966673296689988, 0.14743673801422122, 0.7947223049402237, 0.874164292216301, 0.04768068790435792, 0.0, 0.0057275533676147475, 0.5, 0.08289816379547121, 0.021656823158264164, 0.12917234897613528, 0.1107665777206421, 0.8847532254457474, 0.10121519565582278, 0.7345190984010697, 0.5, 0.09699828624725344, 0.10436303615570071, 0.921204132437706, 0.09430110454559328, 1.0, 0.8022554296255112, 0.09191615581512452, 0.041793036460876475, 0.10664355754852296, 0.7286895209550859, 0.12583076953887942, 0.0633220911026001, 0.7498977774381638, 0.7559227102994919]\n"
     ]
    }
   ],
   "source": [
    "shifter = ScoreShifter()\n",
    "shifter.fit(predicted_scores=dev_test_scores, ground_truth_scores=dev_gt_scores)\n",
    "dev_test_scores = shifter.transform(dev_test_scores)\n",
    "print(dev_test_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And again, we are now ready to test the performance of O1 on the test problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/venv/py310/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8125\n",
      "AUC:  0.8899739583333334\n",
      "c@1:  0.830078125\n",
      "AUC x c@1:  0.7387479146321615\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_pan_dataset(D + \"test\")\n",
    "train_labels, train_documents = zip(*train_data)\n",
    "test_labels, test_documents = zip(*test_data)\n",
    "\n",
    "# vectorize:\n",
    "vectorizer = Vectorizer(mfi=10000, vector_space=\"tf\", ngram_type=\"word\", ngram_size=1)\n",
    "train_X = vectorizer.fit_transform(train_documents)\n",
    "test_X = vectorizer.transform(test_documents)\n",
    "\n",
    "# encode author labels:\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels + test_labels)\n",
    "train_y = np.array(label_encoder.transform(train_labels), dtype=\"int\")\n",
    "test_y = np.array(label_encoder.transform(test_labels), dtype=\"int\")\n",
    "\n",
    "# fit and predict a verifier on the test data:\n",
    "test_verifier = Order1Verifier(metric=\"minmax\", base=\"profile\")\n",
    "test_verifier.fit(train_X, train_y)\n",
    "test_scores = test_verifier.predict_proba(test_X=test_X, test_y=test_y)\n",
    "\n",
    "# load the ground truth:\n",
    "test_gt_scores = load_ground_truth(\n",
    "    filepath=os.sep.join((D, \"test\", \"truth.txt\")), labels=test_labels\n",
    ")\n",
    "\n",
    "# apply the optimzed score shifter:\n",
    "test_scores = shifter.transform(test_scores)\n",
    "\n",
    "test_acc_score, test_auc_score, test_c_at_1_score = pan_metrics(\n",
    "    prediction_scores=test_scores, ground_truth_scores=test_gt_scores\n",
    ")\n",
    "\n",
    "print(\"Accuracy: \", test_acc_score)\n",
    "print(\"AUC: \", test_auc_score)\n",
    "print(\"c@1: \", test_c_at_1_score)\n",
    "print(\"AUC x c@1: \", test_auc_score * test_c_at_1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
